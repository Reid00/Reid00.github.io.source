---
title: "数据降维之主成分分析 PCA"
date: 2022-06-07T19:28:59+08:00
lastmod: 2022-06-07T19:28:59+08:00
author: ["Reid"]
categories: 
- Machine Learning
tags: 
- feature engineering
keyword:
- Machine Learning
- 机器学习
- PCA
- 主成分分析
description: ""
weight: # 输入1可以顶置文章，用来给文章展示排序，不填就默认按时间排序
slug: ""
draft: false # 是否为草稿
comments: true
showToc: true # 显示目录
TocOpen: false # 自动展开目录
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: true # 底部不显示分享栏
showbreadcrumbs: true #顶部显示当前路径
cover:
    image: ""
    caption: ""
    alt: ""
    relative: false
---

## Summary  

PCA 是无监督学习中最常见的数据降维方法，但是实际上问题特征很多的情况，PCA通常会预处理来减少特征个数。

>将维的意义：
> 通过降维提高算法的效率
>通过降维更方便数据的可视化，通过可视化我们可以更好的理解数据

---

## 相关统计概念

- 均值： 述的是样本集合的中间点。
- 方差： 概率论和统计方差衡量随机变量或一组数据时离散程度的度量。
- 标准差：而标准差给我们描述的是样本集合的各个样本点到均值的距离之平均。方差开根号。
- **标准差和方差一般是用来描述一维数据的**
- 协方差: （多维）度量两个随机变量关系的统计量,来度量各个维度偏离其均值的程度。
- 协方差矩阵: （多维）度量各个维度偏离其均值的程度

>- 当 cov(X, Y)>0时，表明X与Y正相关(X越大，Y也越大；X越小Y，也越小。)
>- 当 cov(X, Y)<0时，表明X与Y负相关；
>- 当 cov(X, Y)=0时，表明X与Y不相关。
>- cov协方差=[(x1-x均值)*(y1-y均值)+(x2-x均值)*(y2-y均值)+...+(xn-x均值)*(yn-y均值)]/(n-1)

---

## PCA 思想

>- 对数据进行归一化处理（代码中并非这么做的，而是直接减去均值）
>- 计算归一化后的数据集的协方差矩阵
>- 计算协方差矩阵的特征值和特征向量
>- 将特征值排序
>- 保留前N个最大的特征值对应的特征向量
>- 将数据转换到上面得到的N个特征向量构建的新空间中（实现了特征压缩）

## 简述主成分分析PCA工作原理，以及PCA的优缺点？

 PCA旨在找到数据中的主成分，并利用这些主成分表征原始数据，从而达到降维的目的。

​    工作原理可由两个角度解释，第一个是最大化投影方差（让数据在主轴上投影的方差尽可能大）；第二个是最小化平方误差（样本点到超平面的垂直距离足够近）。

​    做法是数据中心化之后，对样本数据协方差矩阵进行特征分解，选取前d个最大的特征值对应的特征向量，即可将数据从原来的p维降到d维，也可根据奇异值分解来求解主成分。 

### 优点：

1.计算简单，易于实现

2.各主成分之间正交，可消除原始数据成分间的相互影响的因素

3.仅仅需要以方差衡量信息量，不受数据集以外的因素影响

4.降维维数木有限制，可根据需要制定

### 缺点：

1.无法利用类别的先验信息

2.降维后，只与数据有关，主成分各个维度的含义模糊，不易于解释

3.方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响

4.线性模型，对于复杂数据集难以处理（可用核映射方式改进）

## PCA中有第一主成分、第二主成分，它们分别是什么，又是如何确定的？

主成分分析是设法将原来众多具有一定相关性（比如P个指标），重新组合成一组新的互相无关的综合指标来代替原来的指标。主成分分析，是考察多个变量间相关性一种多元统计方法，研究如何通过少数几个主成分来揭示多个变量间的内部结构，即从原始变量中导出少数几个主成分，使它们尽可能多地保留原始变量的信息，且彼此间互不相关，通常数学上的处理就是将原来P个指标作线性组合，作为新的综合指标。 

​    最经典的做法就是用F1（选取的第一个线性组合，即第一个综合指标）的方差来表达，即Var(F1)越大，表示F1包含的信息越多。因此在所有的线性组合中选取的F1应该是方差最大的，故称F1为第一主成分。如果第一主成分不足以代表原来P个指标的信息，再考虑选取F2即选第二个线性组合，为了有效地反映原来信息，F1已有的信息就不需要再出现在F2中，用数学语言表达就是要求Cov(F1, F2)=0，则称F2为第二主成分，依此类推可以构造出第三、第四，……，第P个主成分。



## LDA与PCA都是常用的降维方法，二者的区别

它其实是对数据在高维空间下的一个投影转换，通过一定的投影规则将原来从一个角度看到的多个维度映射成较少的维度。到底什么是映射，下面的图就可以很好地解释这个问题——正常角度看是两个半椭圆形分布的数据集，但经过旋转（映射）之后是两条线性分布数据集。

**LDA与PCA都是常用的降维方法，二者的区别在于：**

**出发思想不同。**PCA主要是从特征的协方差角度，去找到比较好的投影方式，即选择样本点投影具有最大方差的方向（ 在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。）；而LDA则更多的是考虑了分类标签信息，寻求投影后不同类别之间数据点距离更大化以及同一类别数据点距离最小化，即选择分类性能最好的方向。

**学习模式不同。**PCA属于无监督式学习，因此大多场景下只作为数据处理过程的一部分，需要与其他算法结合使用，例如将PCA与聚类、判别分析、回归分析等组合使用；LDA是一种监督式学习方法，本身除了可以降维外，还可以进行预测应用，因此既可以组合其他模型一起使用，也可以独立使用。

**降维后可用维度数量不同。**LDA降维后最多可生成C-1维子空间（分类标签数-1），因此LDA与原始维度N数量无关，只有数据标签分类数量有关；而PCA最多有n维度可用，即最大可以选择全部可用维度。

**线性判别分析LDA算法由于其简单有效性在多个领域都得到了广泛地应用，是目前机器学习、数据挖掘领域经典且热门的一个算法；但是算法本身仍然存在一些局限性：**

当样本数量远小于样本的特征维数，样本与样本之间的距离变大使得距离度量失效，使LDA算法中的类内、类间离散度矩阵奇异，不能得到最优的投影方向，在人脸识别领域中表现得尤为突出

LDA不适合对非高斯分布的样本进行降维

LDA在样本分类信息依赖方差而不是均值时，效果不好

LDA可能过度拟合数据



## 主成分分析 PCA 详解

### 原理及对应操作

主成分分析顾名思义是对主成分进行分析，那么找出主成分应该是key点。PCA的基本思想就是将初始数据集中的n维特征映射至k维上，得到的k维特征就可以被称作主成分，k维不是在n维中挑选出来的，而是以n维特征为基础重构出来的。

PCA会在已知数据的基础上重构坐标轴，它的原理是要最大化保留样本间的方差，两个特征之间方差越大不就代表相关性越差嘛。比如：

- 第一个新坐标轴就是原始数据中方差最大的方向。
- 第二个新坐标轴要是与第一个新坐标轴正交的平面中方差最大的方向。
- 第三个新坐标轴要是与第一、第二新坐标轴正交的平面中方差最大的方向。
- 第四、第五...依次类推直到第n个新坐标轴(对应n维)。

为了加深这部分理解，以二维平面先举一个例子，二维平面中依据原始数据新建坐标轴如下图：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/R4or34uBPGjEiabJ7bl2MFEaKIFSia60y933DtuPdND89JCAOqLkTJ7MECDgwKIOo1kAr7Nc2WVVX1cR85gFGnMw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

为了更直观的理解，若将方差换一个说法，那么第一个新坐标轴就是覆盖样本最多的一条(斜向右上)，第二个新坐标轴需要与第一新坐标轴正交且覆盖样本最多(斜向左上)，依次类推。

覆盖的样本多少并不是以坐标轴穿过多少样本点评判的，而是通过样本点垂直映射至该轴的个数有多少，具体如下图：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/R4or34uBPGjEiabJ7bl2MFEaKIFSia60y9AxmCqoiaibLwO50nRoXjQBBpahqrGTbia1LpciaLibmiaU8DsufRkM10oXwA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

回到之前的n维重构坐标轴，由于顺序是依据方差的大小依次排序的，所以越到后面方差越小，而方差越小代表特征之间相关性越强，那么这类特征就可以删去，只保留前k个坐标轴(对应k维)，这就相当于保留了含有数据集绝大部分方差的特征，而除去方差几乎为0的特征。

那么问题来了，二维、三维可以根据样本点的分布画出重构坐标轴，但是更高维人的大脑是不接受的，我们不得不通过计算的方式求得特征之间的方差，进而得到这些新坐标轴的的方向。

具体方法就是通过计算原始数据矩阵对应的协方差矩阵，然后可以得到协方差矩阵对应的特征值和特征向量，选取特征值最大的前k个特征向量组成的矩阵，通过特征矩阵就可以将原始数据矩阵从n维空间映射至k维空间，从而实现特征降维。


### 方差、协方差及协方差矩阵

如果你曾经接触过线性代数可能对这三个概念很熟悉，可能间隔时间太久有些模糊，下面再帮大家温习一下：

方差(Variance)一般用来描述样本集合中的各个样本点到样本均值的差的平方和的均值：



协方差(Covariance)目的是度量两个变量(只能为两个)线性相关的程度：



为可以说明两个变量线性无关，但不能证明两个变量相互独立，当时，二者呈正相关，时，二者呈负相关。

协方差矩阵就是由两两变量之间的协方差组成，有以下特点：

- 协方差矩阵可以处理多维度问题。
- 协方差矩阵是一个对称的矩阵，而且对角线是各个维度上的方差。
- 协方差矩阵计算的是不同维度之间的协方差，而不是不同样本之间的。
- 样本矩阵中若每行是一个样本，则每列为一个维度。

假设数据是3维的，那么对应协方差矩阵为：



这里简要概括一下协方差矩阵是怎么求得的，假设一个数据集有3维特征、每个特征有m个变量，这个数据集对应的数据矩阵如下：



若假设他们的均值都为0，可以得到下面等式：

![img](https://mmbiz.qpic.cn/mmbiz_png/R4or34uBPGgVLuwa5iaF4zyEkfvpCxKziafngZAw8I5ia3DGuALxnMaTjQDIosP6bWldbgMS2Do7tB1L4dFS47RMQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可以看到对角线上为每个特征方差，其余位置为两个特征之间的协方差，求得的就为协方差矩阵。

> 这里叙述的有些简略，感兴趣的小伙伴可以自行查询相关知识。

### 特征值和特征向量

得到了数据矩阵的协方差矩阵，下面就应该求协方差矩阵的特征值和特征向量，先了解一下这两个概念，如果一个向量v是矩阵A的特征向量，那么一定存在下列等式：



其中A可视为数据矩阵对应的协方差矩阵，是特征向量v的特征值。数据矩阵的主成分就是由其对应的协方差矩阵的特征向量，按照对应的特征值由大到小排序得到的。最大的特征值对应的特征向量就为第一主成分，第二大的特征值对应的特征向量就为第二主成分，依次类推，如果由n维映射至k维就截取至第k主成分。

## 实例操作

通过上述部分总结一下PCA降维操作的步骤：

1. 去均值化
2. 依据数据矩阵计算协方差矩阵
3. 计算协方差矩阵的特征值和特征向量
4. 将特征值从大到小排序
5. 保留前k个特征值对应的特征向量
6. 将原始数据的n维映射至k维中

### 公式手推降维

原始数据集矩阵，每行代表一个特征:



对每个特征去均值化：



计算对应的协方差矩阵：

![img](https://mmbiz.qpic.cn/mmbiz_png/R4or34uBPGgVLuwa5iaF4zyEkfvpCxKziaicJyZc91SsWPqfctMNepLsvw0GaX1SiajUFKG55h44ebN6R2jXhlwZww/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

依据协方差矩阵计算特征值和特征向量，套入公式：



拆开计算如下：

![img](https://mmbiz.qpic.cn/mmbiz_png/R4or34uBPGgVLuwa5iaF4zyEkfvpCxKzianGiaVpvMCAGAvFMHdANlIpUias5ytAEzgoWmu3icibq2s8l5D8fQuu2KpA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可以求得两特征值：

，

当时，对应向量应该满足如下等式：



对应的特征向量可取为：





同理当时，对应特征向量可取为：





这里我就不对两个特征向量进行标准化处理了，直接合并两个特征向量可以得到矩阵P：



选取大的特征值对应的特征向量乘以原数据矩阵后可得到降维后的矩阵A：

![img](https://mmbiz.qpic.cn/mmbiz_png/R4or34uBPGgVLuwa5iaF4zyEkfvpCxKziakPfYBiao6C8NZqRajic3TmKSm8CnvXybdcaiaM4D3YHKLSFI6ek1mPq2w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

综上步骤就是通过PCA手推公式实现二维降为一维的操作。

### numpy实现降维

```
import numpy as np

def PCA(X,k):
    '''
    X:输入矩阵
    k:需要降低到的维数
    '''
    X = np.array(X) #转为矩阵
    sample_num ,feature_num = X.shape #样本数和特征数
    meanVals = np.mean(X,axis = 0) #求每个特征的均值
    X_mean = X-meanVals #去均值化
    Cov = np.dot(X_mean.T,X_mean)/sample_num #求协方差矩阵
    feature_val,feature_vec = np.linalg.eig(Cov)
    #将特征值和特征向量打包
    val_sort = [(np.abs(feature_val[i]),feature_vec[:,i]) for i in range(feature_num)]
    val_sort.sort(reverse=True) #按特征值由大到小排列
    #截取至前k个特征向量组成特征矩阵
    feature_mat = [feature[1] for feature in val_sort[:k]]
    # 降n维映射至k维
    PCA_mat = np.dot(X_mean,np.array(feature_mat).T)
    return PCA_mat
if __name__ == "__main__":
    X = [[1, 1], [1, 3], [2, 3], [4, 4], [2, 4]]
    print(PCA(X,1))
```

运行截图如下：

![img](https://mmbiz.qpic.cn/mmbiz_png/R4or34uBPGjEiabJ7bl2MFEaKIFSia60y9jwaia3EQjx8yMMAR0UdnicAd3Fv8f2aKzdmLuY4mmCVmWHlEK4LNdkdA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

代码部分就是公式的套用，每一步后都有注释，不再过多解释。可以看到得到的结果和上面手推公式得到的有些出入，上文曾提过特征向量是可以随意缩放的，这也是导致两个结果不同的原因，可以在运行代码时打印一下特征向量feature_vec，观察一下这个特点。

### sklearn库实现降维

```
from sklearn.decomposition import PCA
import numpy as np

X = [[1, 1], [1, 3], [2, 3], [4, 4], [2, 4]]
X = np.array(X)
pca = PCA(n_components=1)
PCA_mat = pca.fit_transform(X)
print(PCA_mat)
```

这里只说一下参数n_components，如果输入的是整数，代表数据集需要映射的维数，比如输入3代表最后要映射至3维；如果输入的是小数，则代表映射的维数为原数据维数的占比，比如输入0.3，如果原数据20维，就将其映射至6维。