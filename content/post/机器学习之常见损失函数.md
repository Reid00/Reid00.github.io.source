---
title: "机器学习之常见损失函数"
date: 2022-06-08T11:08:15+08:00
lastmod: 2022-06-08T11:08:15+08:00
author: ["Reid"]
categories: 
- Machine Learning
tags: 
- 损失函数
keyword:
- Machine Learning
- 损失函数
description: ""
weight: # 输入1可以顶置文章，用来给文章展示排序，不填就默认按时间排序
slug: ""
draft: false # 是否为草稿
comments: true
showToc: true # 显示目录
TocOpen: false # 自动展开目录
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: true # 底部不显示分享栏
showbreadcrumbs: true #顶部显示当前路径
cover:
    image: ""
    caption: ""
    alt: ""
    relative: false
---
###  简介

损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。

损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。

常见的损失函数以及其优缺点如下：

### 1. 0-1损失函数(zero-one loss)

0-1损失是指预测值和目标值不相等为1， 否则为0:

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PAHWcPnIKojZaRNiapm2gQ9vL5TibKIf6TgvRdF07gTuERYt7ibFuZdaBmCZ93rxqbDW/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

特点：

(1) 0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用.

(2) 感知机就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PEiaZXEibljgib0JFSYUclfOQOwu9wZSsUEUmibGlrPAy9mSJFDzruS7gFclUxgHWPYEv/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 时认为相等，

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PfUUbMY31Mq3Mam4jlopo9zsiaXGqbLYcmAlzJwOQ9dHX7VH05FCFCXWxib3iauFFoY9/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

### 2. 绝对值损失函数

绝对值损失函数是计算预测值与目标值的差的绝对值：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PqqNxtic3kLa4vYcib5nNB3lTMHEDjPXAElB9xoXs0aeBhuStmETzxVS0EPzSfA0uWh/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

### 3. log对数损失函数

log对数损失函数的标准形式如下：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PpJ6R3pqjoFLLrTWaVfytyrJ6H4Hcw3wTjcqnCQhry1h3toP6RuGjyLSRaDJ74Nqn/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

特点：

(1) log对数损失函数能非常好的表征概率分布，在很多场景尤其是多分类，如果需要知道结果属于每个类别的置信度，那它非常适合。

(2) 健壮性不强，相比于hinge loss对噪声更敏感。

(3) 辑回归的损失函数就是log对数损失函数。

### 4. 平方损失函数

平方损失函数标准形式如下：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423P579bRBVDZH3kdnV8ng8iaWqDDRsuhoyRd7ehbn5PbiaAoq9SMz3JSYMKCJLOmsmNAo/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

特点：

(1)经常应用与回归问题

### 5. 指数损失函数（exponential loss）

指数损失函数的标准形式如下：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PyEGzwtbyLibWJAlgstoO1SoX8lepgkt73esz0nht681l5icwQ4l9DVyTUCUNibnBOh0/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

特点：

(1)对离群点、噪声非常敏感。经常用在AdaBoost算法中。



### 6. Hinge 损失函数

Hinge损失函数标准形式如下：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PCOLBC18NUlBAE7BLicXibNA32tRmScoDXF0GLbvgn7k3LzQ555HHdKPcwwFUK6eMt5/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

特点：

(1) hinge损失函数表示如果被分类正确，损失为0，否则损失就为 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PyXKaGP2GxfmgicevnHDTBlic7E4d6Oremd0MfF8mmtQmCAlBuZIGBogV9vF5X9Tt8p/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 。SVM就是使用这个损失函数。

(2) 一般的 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PibEDGpaLkvqynkmNCpWrKhzwNvw130IjjjMkh7nOiaFEsQ97r8ZsicfGGhDAhjKLDxw/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 是预测值，在-1到1之间， ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423Pn2s1e1VW5syWX3WzKUtnGQ3ecib4ibZLtCfUjg8aHd8n57ypJr14yhHOwF4ZJW0rFu/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 是目标值(-1或1)。其含义是， ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PibEDGpaLkvqynkmNCpWrKhzwNvw130IjjjMkh7nOiaFEsQ97r8ZsicfGGhDAhjKLDxw/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 的值在-1和+1之间就可以了，并不鼓励 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PIKCdeIsCNfaOkGPZkh0fNfKGvN1H1zV5N3xMUXZVoxoNGCGUFqbzhlh8k16AlwPA/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) ，即并不鼓励分类器过度自信，让某个正确分类的样本距离分割线超过1并不会有任何奖励，从而使分类器可以更专注于整体的误差。

(3) 健壮性相对较高，对异常点、噪声不敏感，但它没太好的概率解释。



### 7. 感知损失(perceptron loss)函数

感知损失函数的标准形式如下：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PqP49Gg8aBhZhM93j7NntWbicoQfL6CVmrlAlicZsJxsF46nD2BPw0rYrb47nfYeena/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

特点：

(1)是Hinge损失函数的一个变种，Hinge loss对判定边界附近的点(正确端)惩罚力度很高。而perceptron loss只要样本的判定类别正确的话，它就满意，不管其判定边界的距离。它比Hinge loss简单，因为不是max-margin boundary，所以模型的泛化能力没 hinge loss强。



### 8. 交叉熵损失函数 (Cross-entropy loss function)

交叉熵损失函数的标准形式如下:

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PXrovpAAhG5B3VN8fR2BDQcGVRPMFSnbUAt0lRIicTkM1CQE06IcrMFGicBGBHiaQmP7/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

注意公式中 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PzMcQdCjSaPKEsuGA3qVia2V01iaiavG9vqGrnXGIU4bAFyDzRAcI1ziakGdGibxf9h480/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 表示样本， ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423Pn2s1e1VW5syWX3WzKUtnGQ3ecib4ibZLtCfUjg8aHd8n57ypJr14yhHOwF4ZJW0rFu/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 表示实际的标签， ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PpKZVbiaplnUZSB9d5x53qDXYwv0qv0aIyqoWqUT5tZxZyISBzJftiaR9Rl6TkeARuz/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 表示预测的输出， ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PFGMYb5H2ZiaWic0iad06zycVSAugTlBWY8gTqn2nwau6MuTu4dK2RuS43y8qgp4yyo3/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 表示样本总数量。

特点：

(1)本质上也是一种对数似然函数，可用于二分类和多分类任务中。

二分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PKrdLEmnP7jU3ITsicjmcibLeJcdYKdb6xyt4wOQX6KltU8R6XXNddyYHic4MAbUibach/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

多分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423P4wq5FsjAPz3LVIsG737np2C93hW27L7O0dkiahjotq5nHic5lfjlwuNGib2Z34O3869/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

(2)当使用sigmoid作为激活函数的时候，常用交叉熵损失函数而不用均方误差损失函数，因为它可以完美解决平方损失函数权重更新过慢的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。

最后奉献上交叉熵损失函数的实现代码：cross_entropy.



------

这里需要更正一点，对数损失函数和交叉熵损失函数应该是等价的！！！（此处感谢 

@Areshyy

 的指正，下面说明也是由他提供）



下面来具体说明：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/nJZZib3qIQW4utAZSNewpcT9VHQXxgcdul66FX1VTXeIAC8nSm5BqR1eYp72ibDEAmDGCUC99jHI2vdrFdW4wfNQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



### 相关高频问题：

#### 1.交叉熵函数与最大似然函数的联系和区别？

区别：交叉熵函数使用来描述模型预测值和真实值的差距大小，越大代表越不相近；似然函数的本质就是衡量在某个参数下，整体的估计和真实的情况一样的概率，越大代表越相近。

联系：交叉熵函数可以由最大似然函数在伯努利分布的条件下推导出来，或者说最小化交叉熵函数的本质就是对数似然函数的最大化。

怎么推导的呢？我们具体来看一下。

设一个随机变量 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PnuQMlBzdwEh5l9f1rMMlUEEMtvBZJl74pLxo9QUU35GOwmdRjykH502aZWBhGxmO/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 满足伯努利分布，

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423P7DgvTgxhorZuacuzVMzD2RC76VP5ibuIy5NTib78JAWibKPxmOncW5KVPWtR4GNdFrb/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

则 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PnuQMlBzdwEh5l9f1rMMlUEEMtvBZJl74pLxo9QUU35GOwmdRjykH502aZWBhGxmO/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 的概率密度函数为：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PxNyATb3sIZtaEa0UAJXungHlkGfibMxeQd8gOH9XZQOCnHYgcZtfQXq20pr2CM79t/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

因为我们只有一组采样数据 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423P4vsqzRcgClhJ3jqiawogELMHIiaxscOMpRpofibLIntWPFOAqJataDPXC6jKXiarqaeg/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) ，我们可以统计得到 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PnuQMlBzdwEh5l9f1rMMlUEEMtvBZJl74pLxo9QUU35GOwmdRjykH502aZWBhGxmO/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 和 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PPUBLkgicLsR2NBUTccmRUrYlYoicPE3A5qZxebBu708UDCbicsgqe42EnFciciaibyBu6u/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 的值，但是 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PNx8Tvcrc1cDXRXs7aZAvLaJYaOoxJ1qTcVUY2ib7c6shc9jaljpickcIc4iaUG1MLkn/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 的概率是未知的，接下来我们就用极大似然估计的方法来估计这个 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PNx8Tvcrc1cDXRXs7aZAvLaJYaOoxJ1qTcVUY2ib7c6shc9jaljpickcIc4iaUG1MLkn/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 值。

对于采样数据 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423P4vsqzRcgClhJ3jqiawogELMHIiaxscOMpRpofibLIntWPFOAqJataDPXC6jKXiarqaeg/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) ，其对数似然函数为:

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PicTZUMfP251kiabm571PwDhr4LcMekP32tmpb57HgiaKgVtsDHicapU56rR5vhia4HHiaj/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可以看到上式和交叉熵函数的形式几乎相同，极大似然估计就是要求这个式子的最大值。而由于上面函数的值总是小于0，一般像神经网络等对于损失函数会用最小化的方法进行优化，所以一般会在前面加一个负号，得到交叉熵函数（或交叉熵损失函数）：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423Pahk7Bfhbwly5YXibzcRnV9uX5Xb40mcQZmwNiab5icP7D9nxl95icuz0VI2gQhg2Gpck/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这个式子揭示了交叉熵函数与极大似然估计的联系，最小化交叉熵函数的本质就是对数似然函数的最大化。

现在我们可以用求导得到极大值点的方法来求其极大似然估计，首先将对数似然函数对 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PNx8Tvcrc1cDXRXs7aZAvLaJYaOoxJ1qTcVUY2ib7c6shc9jaljpickcIc4iaUG1MLkn/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 进行求导，并令导数为0，得到

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PibJaDKzogq6wf9O0vU9yC8LhUqTw0yZYPXRBlNzDiaTtG0IEbe0eh1PBcHrtpKO4A2/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

消去分母，得：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PiajibtOLUrQrEAsXIMX44v81G8qBvj6uibXfy7ZIHCFactRlrXoGmibt8DLWPP8Ll9NU/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

所以:

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PL9Vt79xuC2dgJo8IwmSxQWqFsOUvBnpWY5vSxzvn64Uh3qXibWDmnDjNKVic78OJ9v/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这就是伯努利分布下最大似然估计求出的概率 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PNx8Tvcrc1cDXRXs7aZAvLaJYaOoxJ1qTcVUY2ib7c6shc9jaljpickcIc4iaUG1MLkn/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 。

#### 2. 在用sigmoid作为激活函数的时候，为什么要用交叉熵损失函数，而不用均方误差损失函数？

其实这个问题求个导，分析一下两个误差函数的参数更新过程就会发现原因了。

对于均方误差损失函数，常常定义为：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PCg3P8SWeNrO6ib3sMMZfEibW6d9QFV5RnYmcUWTz6OiaNIkAmFgddupggGbwxWhWzO5/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423Pn2s1e1VW5syWX3WzKUtnGQ3ecib4ibZLtCfUjg8aHd8n57ypJr14yhHOwF4ZJW0rFu/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 是我们期望的输出， ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PpKZVbiaplnUZSB9d5x53qDXYwv0qv0aIyqoWqUT5tZxZyISBzJftiaR9Rl6TkeARuz/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 为神经元的实际输出（ ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PL1Lzf4T6Hoia1Xmib2xpo2S9GTlvwj7Df5R42KfFRd78SQsGgfBJgvlQMvkVE1ic3zj/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) ）。在训练神经网络的时候我们使用梯度下降的方法来更新 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PrVCdNOJK2m87tFKH1TWX841lwd6AfiaP4psKhk0ZHkTaEBfPo2JOevjzicAh8eEvKic/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 和 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PTIXUib7GqicHcJmDHOldZ7hjl12GpMMKRJfNBpfUxHcRibC3fNMMka37zxDKztO2Jmic/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) ，因此需要计算代价函数对 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PrVCdNOJK2m87tFKH1TWX841lwd6AfiaP4psKhk0ZHkTaEBfPo2JOevjzicAh8eEvKic/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 和 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PTIXUib7GqicHcJmDHOldZ7hjl12GpMMKRJfNBpfUxHcRibC3fNMMka37zxDKztO2Jmic/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 的导数：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PDm01kHIeko6Afxa3yDgzyFER14ibnRZ1jic47LNsvUncJUl1TW3VhiaHpM38TPhGxRZ/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

然后更新参数 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PrVCdNOJK2m87tFKH1TWX841lwd6AfiaP4psKhk0ZHkTaEBfPo2JOevjzicAh8eEvKic/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 和 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PTIXUib7GqicHcJmDHOldZ7hjl12GpMMKRJfNBpfUxHcRibC3fNMMka37zxDKztO2Jmic/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) ：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PFNBMUvfhvdlFEOILkNBia2Bhp3gHU1BXCXHL9PEx4icMVrLqxP7Gm1KpvKIjibkicxRm/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

因为sigmoid的性质，导致 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PQpbFhwuoiampfAGKRDfC4jc8N5mUy4yROhzcJrD2TRMn3PAFutX2TVauFgYlt93U2/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 在 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PnFNNkISGj0pRAEUsWmqaiaTTf1WLjgUoN3LXJH7Apib1ibJtxiaZBHTGIApQRjpJqqGY/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 取大部分值时会很小（如下图标出来的两端，几乎接近于平坦），这样会使得 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423P6Yu8racxticu6sSvic55owan6B5CP8zHNhC5rSyvAO9tUx7zQORZTy0YrZB6hTX3Ub/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 很小，导致参数 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PrVCdNOJK2m87tFKH1TWX841lwd6AfiaP4psKhk0ZHkTaEBfPo2JOevjzicAh8eEvKic/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 和 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PTIXUib7GqicHcJmDHOldZ7hjl12GpMMKRJfNBpfUxHcRibC3fNMMka37zxDKztO2Jmic/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 更新非常慢。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/nJZZib3qIQW4utAZSNewpcT9VHQXxgcduALUh9Ml3Z0lV7XL0pkrUaj7X2nF7y82Yl7dMAKxe1OxicrfRPEpmWsA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

那么为什么交叉熵损失函数就会比较好了呢？同样的对于交叉熵损失函数，计算一下参数更新的梯度公式就会发现原因。交叉熵损失函数一般定义为：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PXrovpAAhG5B3VN8fR2BDQcGVRPMFSnbUAt0lRIicTkM1CQE06IcrMFGicBGBHiaQmP7/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

其中 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423Pn2s1e1VW5syWX3WzKUtnGQ3ecib4ibZLtCfUjg8aHd8n57ypJr14yhHOwF4ZJW0rFu/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 是我们期望的输出， ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PpKZVbiaplnUZSB9d5x53qDXYwv0qv0aIyqoWqUT5tZxZyISBzJftiaR9Rl6TkeARuz/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 为神经元的实际输出（ ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PL1Lzf4T6Hoia1Xmib2xpo2S9GTlvwj7Df5R42KfFRd78SQsGgfBJgvlQMvkVE1ic3zj/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) ）。同样可以看看它的导数：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423Pahk7BfhbwlzgGUsXaPkPXD7lSqf888oj1Mw4wXr9tvpS32EngppHZ67LQ9aQKye1/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

另外，

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PMTPNdT9MKZ7ZiacKMxwfNuSiaDQTeFcycZQuu5fiasS82hg3OHHOb7v5WjPYpdInux9/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

所以有：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PqhNlauSG2kdS1Ija8z8LtOnSGZjLBB8ORTVMRgcUiapEbnPkI14MBDNibrMsUUnYbc/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PFZejk3NKYXFo1ZvKeJCYDD62uycTHEloAdlImalafNChDOA4ngibgqsca8QLJeukq/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

所以参数更新公式为：

![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423P5UWVKPmoa3bTkibC61ajRIj6PyRx3V3mOC3cZwTSLcGAtmB7QsYX8IPOOvCv7ib9tM/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可以看到参数更新公式中没有 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PQpbFhwuoiampfAGKRDfC4jc8N5mUy4yROhzcJrD2TRMn3PAFutX2TVauFgYlt93U2/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 这一项，权重的更新受 ![img](https://mmbiz.qpic.cn/mmbiz_svg/6t0VDe9bl5f74XMG3ea8ibrDymbNS423PgDF8dZGdicIMd8diaVvgsk1crpUzhEAplssiahhKIebibtzObrWaBIXSORT4B3D9TSx7/640?wx_fmt=svg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 影响，受到误差的影响，**所以*当误差大的时候，权重更新快；当误差小的时候，权重更新慢***。这是一个很好的性质。

所以当使用sigmoid作为激活函数的时候，常用交叉熵损失函数而不用均方误差损失函数。