---
title: "20230214 MIT6.824 2022 Lab4 ShardedKV"
date: 2023-02-20T17:45:45+08:00
lastmod: 2023-02-20T17:45:45+08:00
author: ["Reid"]
categories: 
- Storage
- Raft
tags: 
- Raft
- MIT6.824
- Consensus
- 共识算法
- ShardedKV
keyword:
- Storage
- Raft
- MIT6.824
- Consensus
- 共识算法
- ShardedKV
description: ""
weight: # 输入1可以顶置文章，用来给文章展示排序，不填就默认按时间排序
slug: 20230214-MIT6.824-2022-Lab4-ShardedKV
draft: false # 是否为草稿
comments: true
showToc: true # 显示目录
TocOpen: false # 自动展开目录
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: true # 底部不显示分享栏
showbreadcrumbs: true #顶部显示当前路径
cover:
    image: ""
    caption: ""
    alt: ""
    relative: false
---

# ShardedKV 介绍
有关 shardkv，其可以算是一个 multi-raft 的实现，只是缺少了物理节点的抽象概念。在实际的生产系统中，不同 raft 组的成员可能存在于一个物理节点上，而且一般情况下都是一个物理节点拥有一个状态机，不同 raft 组使用不同地命名空间或前缀来操作同一个状态机。基于此，下文所提到的的节点都代指 raft 组的某个成员，而不代指某个物理节点。比如节点宕机代指 raft 组的某个成员被 kill 掉，而不是指某个物理节点宕机，从而可能影响多个 raft 的成员。

在本实验中，我们将构建一个带分片的KV存储系统，即一组副本组上的键。每一个分片都是KV对的子集，例如，所有以“a”开头的键可能是一个分片，所有以“b”开头的键可能是另一个分片。 也可以用range 或者Hash 之后分区。
分片的原因是性能。每个replica group只处理几个分片的 put 和 get，并且这些组并行操作；因此，系统总吞吐量（每单位时间的投入和获取）与组数成比例增加。

我们的整个系统有两个基本组件：shard controller 和 shard group。整个系统有一个 controller 和多个 group，controller 单独一个 raft 集群，每一个 shard group 是由 kvraft 实例构成的集群。shard controller 负责调度，客户端向 shard controller 发送请求，controller 会根据配置(config)来告知客户端服务这个 key 的是哪个 group。 每个 group 负责部分 shard。

```go
type Config struct {
	Num    int              // config number, version also
	Shards [NShards]int     // shard -> gid
	Groups map[int][]string // gid -> servers[]
}
```
三个参数分别对应的版本的配置号，分片所对应的组(Group)信息（实验中的分片为10个），每个组对应的服务器映射名称列表（也就是组信息）。

Group表示一个Leader-Followers集群，Gid为它的标识，Shard表示所有数据的一个子集，Config表示一个划分方案。此次实验中，所有数据分为NShards = 10份，Server给测试程序提供四个接口。
下图中每个Shard 都有其他对应的副本未画出。对Client 以Group 为单位进行服务。相当于一个物理节点上 有若干个Group 可以对外服务。

![group](https://cdn.staticaly.com/gh/Reid00/image-host@main/20230221/image.7h27rxkk7co0.webp)

分片存储系统必须能够在replica group之间移动分片，因为某些组可能比其他组负载更多，因此需要移动分片以平衡负载；而且replica group可能会加入和离开系统，可能会添加新的副本组以增加容量，或者可能会使现有的副本组脱机以进行修复或报废。

# Lab4A 实现
本实验的主要挑战是处理重新配置——移动分片所属。在单个副本组中，所有组成员必须就何时发生与客户端 Put/Append/Get 请求相关的重新配置达成一致。例如，Put 可能与重新配置大约同时到达，导致副本组停止对该Put包含的key的分片负责。组中的所有副本必须就 Put 发生在重新配置之前还是之后达成一致。如果之前，Put 应该生效，分片的新所有者将看到它的效果；如果之后，Put 将不会生效，客户端必须在新所有者处重新尝试。推荐的方法是让每个副本组使用 Raft 不仅记录 Puts、Appends 和 Gets 的顺序，还记录重新配置的顺序。您需要确保在任何时候最多有一个副本组为每个分片提供请求。

重新配置还需要副本组之间的交互。例如，在配置 10 中，组 G1 可能负责分片 S1。在配置 11 中，组 G2 可能负责分片 S1。在从 10 到 11 的重新配置过程中，G1 和 G2 必须使用 RPC 将分片 S1（键/值对）的内容从 G1 移动到 G2。

Lab4的内容就是将数据按照某种方式分开存储到不同的RAFT集群(Group)上的分片(shard)上。保证相应数据请求引流到对应的集群，降低单一集群的压力，提供更为高效、更为健壮的服务。
![structure](https://cdn.staticaly.com/gh/Reid00/image-host@main/20230221/image.6ikl21aksm80.webp)

- 具体的lab4要实现一个支持 multi-raft分片 、分片数据动态迁移的线性一致性分布式 KV 存储服务。
- shard表示互不相交并且组成完整数据库的每一个数据库子集。group表示shard的集合，包含一个或多个shard。一个shard只可属于一个group，一个group可包含(管理)多个shard。
- lab4A实现`ShardCtrler`服务，作用：提供高可用的集群配置管理服务，实现分片的负载均衡，并尽可能少地移动分片。记录了每组（Group） `ShardKVServer` 的集群信息和每个分片（shard）服务于哪组（Group）`ShardKVServer`。
具体实现通过Raft维护 一个Configs数组，单个config具体内容如下：
    - Num：config number，Num=0表示configuration无效，边界条件， 即是version 的作用
    - Shards：shard -> gid，分片位置信息，Shards[3]=2，说明分片序号为3的分片负贵的集群是Group2（gid=2）
    - Groups：gid -> servers[], 集群成员信息，Group[3]=['server1','server2'],说明gid = 3的集群Group3包含两台名称为server1 & server2的机器

## RPC
- Query RPC。查询配置，参数是一个配置号， `shardctrler` 回复具有该编号的配置。如果该数字为 -1 或大于已知的最大配置数字，则 `shardctrler` 应回复最新配置。 Query(-1) 的结果应该反映 `shardctrler` 在收到 Query(-1) RPC 之前完成处理的每个 Join、Leave 或 Move RPC；

- Join RPC 。添加新的replica group，它的参数是一组从唯一的非零副本组标识符 (GID) 到服务器名称列表的映射。 `shardctrler` 应该通过创建一个包含新副本组的新配置来做出反应。新配置应在所有组中尽可能均匀地分配分片，并应移动尽可能少的分片以实现该目标。如果 GID 不是当前配置的一部分，则 `shardctrler` 应该允许重新使用它（即，应该允许 GID 加入，然后离开，然后再次加入）

新加入的Group信息，要求在每一个group平衡分布shard，即任意两个group之间的shard数目相差不能为1，具体实现每一次找出含有shard数目最多的和最少的，最多的给最少的一个，循环直到满足条件为止。坑为：GID = 0 是无效配置，一开始所有分片分配给GID=0，需要优先分配；map的迭代时无序的，不确定顺序的话，同一个命令在不同节点上计算出来的新配置不一致，按sort排序之后遍历即可。且 map 是引用对象，需要用深拷贝做复制。
> 对于 Join，可以通过多次平均地方式来达到这个目的：每次选择一个拥有 shard 数最多的 raft 组和一个拥有 shard 数最少的 raft，将前者管理的一个 shard 分给后者，周而复始，直到它们之前的差值小于等于 1 且 0 raft 组无 shard 为止。对于 Leave，如果 Leave 后集群中无 raft 组，则将分片所属 raft 组都置为无效的 0；否则将删除 raft 组的分片均匀地分配给仍然存在的 raft 组。通过这样的分配，可以将 shard 分配地十分均匀且产生了几乎最少的迁移任务。

- Leave RPC。删除指定replica group， 参数是以前加入的组的 GID 列表。 `shardctrler` 应该创建一个不包括这些组的新配置，并将这些组的分片分配给剩余的组。新配置应在组之间尽可能均匀地划分分片，并应移动尽可能少的分片以实现该目标；

- Move RPC。移动分片，的参数是一个分片号和一个 GID。 `shardctrler` 应该创建一个新配置，其中将分片分配给组。 Move 的目的是让我们能够测试您的软件。移动后的加入或离开可能会取消移动，因为加入和离开会重新平衡。

```go
// Join according to new Group(gid -> servers) to change the Config
func (cf *MemoryConfigStateMachine) Join(groups map[int][]string) Err {
	lastConfig := cf.Configs[len(cf.Configs)-1]
	newConfig := Config{
		Num:    len(cf.Configs),
		Shards: lastConfig.Shards,
		Groups: deepCopy(lastConfig.Groups),
	}

	for gid, servers := range groups {
		if _, ok := newConfig.Groups[gid]; !ok {
			newServers := make([]string, len(servers))
			copy(newServers, servers)
			newConfig.Groups[gid] = newServers
		}
	}
                                       
	// 找到group 中shard 最大和最小的组，将数据进行move => reblance
	g2s := Group2Shards(newConfig)
	for {
		src, dst := GetGIDWIthMaxShards(g2s), GetGIDWithMinShards(g2s)
		if src != 0 && len(g2s[src])-len(g2s[dst]) <= 1 {
			break
		}

		g2s[dst] = append(g2s[dst], g2s[src][0])
		g2s[src] = g2s[src][1:]
	}

	var newShards [NShards]int
	for gid, shards := range g2s {
		for _, shard := range shards {
			newShards[shard] = gid
		}
	}
	newConfig.Shards = newShards
	cf.Configs = append(cf.Configs, newConfig)
	return OK
}

// Leave some group leave the cluster
func (cf *MemoryConfigStateMachine) Leave(gids []int) Err {
	lastConfig := cf.Configs[len(cf.Configs)-1]
	newConfig := Config{
		Num:    len(cf.Configs),
		Shards: lastConfig.Shards,
		Groups: deepCopy(lastConfig.Groups),
	}

	g2s := Group2Shards(newConfig)
	orphanShards := make([]int, 0)

	for _, gid := range gids {
		delete(newConfig.Groups, gid)
		if shards, ok := g2s[gid]; ok {
			orphanShards = append(orphanShards, shards...)
			delete(g2s, gid)
		}
	}

	var newShards [NShards]int
	if len(newConfig.Groups) != 0 {
		// reblance
		for _, shard := range orphanShards {
			target := GetGIDWithMinShards(g2s)
			g2s[target] = append(g2s[target], shard)
		}

		// update Shards: share -> gid
		for gid, shards := range g2s {
			for _, shard := range shards {
				newShards[shard] = gid
			}
		}
	}

	newConfig.Shards = newShards
	cf.Configs = append(cf.Configs, newConfig)
	return OK
}

// Move move No.shard to No.gid
func (cf *MemoryConfigStateMachine) Move(shard, gid int) Err {
	lastConfig := cf.Configs[len(cf.Configs)-1]
	newConfig := Config{
		Num:    len(cf.Configs),
		Shards: lastConfig.Shards,
		Groups: lastConfig.Groups,
	}

	newConfig.Shards[shard] = gid
	cf.Configs = append(cf.Configs, newConfig)
	return OK
}

// Query return the version of num config
func (cf *MemoryConfigStateMachine) Query(num int) (Config, Err) {
	if num < 0 || num >= len(cf.Configs) {
		return cf.Configs[len(cf.Configs)-1], OK
	}
	return cf.Configs[num], OK
}
```

# Lab4B ShardKV
实验提示:
- 服务器不需要调用分片控制器的Join()，tester 才会去调用；
服务器将需要定期轮询 `shardctrler` 以监听新的配置。预期大约每100毫秒轮询一次；可以更频繁，但过少可能会导致 bug。
- 服务器需要互相发送rpc，以便在配置更改期间传输分片。`shardctrler`的Config结构包含服务器名，一个 Server 需要一个labrpc.ClientEnd，以便发送RPC。使用make_end()函数传给StartServer()函数将服务器名转换为ClientEnd。shardkv /client.go需要实现这些逻辑。
- 在server.go中添加代码去周期性从 `shardctrler` 拉取最新的配置，并且当请求分片不属于自身时，拒绝请求
- 当被请求到错误分片时，需要返回ErrWrongGroup给客户端，并确保Get, Put, Append在面临并发重配置时能正确作出决定
- 重配置需要按流程执行唯一一次
- labgob 的提示错误不能忽视，它可能导致实验不过
- 分片重分配的请求也需要做重复请求检测
- 若客户端收到ErrWrongGroup，是否更改请求序列号? 若服务器执行请求时返回ErrWrongGroup，是否更新客户端信息？
- 当服务器转移到新配置后，它可以继续存储它不再负责的分片（生产环境中这是不允许的），但这个可以简化实现
- 当 G1 在配置变更时需要来自 G2 的分片数据，G2 处理日志条目的哪个时间点将分片发送给 G1 是最好的?
- 你可以在整个 rpc 请求或回复中发送整个 map，这可以简化分片传输
- map 是引用类型，所以在发送 map 的时候，建议先拷贝一次，避免 data race（在 labrpc 框架下，接收 map 时也需要拷贝）
- 在配置更改期间，一对组可能需要互相传送分片，这可能会发生死锁
**challenge**
- 如果想达到生产环境系统级别，如下两个挑战是需要实现的
**challenge1：Garbage collection of state**
当一个副本组失去一个分片的所有权时，副本组需要删除该分片数据。但这给迁移带来一些问题，考虑两个组G1 和 G2，并且新配置C 将分片从 G1 移动到 G2，若 G1 在转换配置到C时删除了数据库中的分片，当G2 转换到C时，如何获取 G1 的数据
**实验要求**
使每个副本组保留旧分片的时长不再是无限时长，即使副本组(如上面的G1)中的所有服务器崩溃并恢复正常，解决方案也必须工作。如果您通过TestChallenge1Delete，您就完成了这个挑战。
**解决方案**
分片迁移成功之后，立马进行分片 GC 了，GC 完毕后再进入到配置更新阶段。
**chanllenge2：Client requests during configuration changes**
配置更改期间最简单的方式是禁止所有客户端操作直到转换完成，虽然简单但是不满足于生产环境要求，这将导致客户端长时间停滞，最好可以继续为不受当前配置更改的分片提供服务
上述优化还能更好，若 G3 在过渡到配置C时，需要来自G1 的分片S1 和 G2 的分片S2。希望 G3 能在收到其中一个分片后可以立即开始接收针对该分片的请求。如G1宕机了，G3在收到G2的分片数据后，可以立即为 S2 分片提供服务，而不需要等待 C 配置转换完全完成
**实验要求**
修改您的解决方案，以便在配置更改期间继续执行不受影响的分片中的 key 的客户端操作。当您通过 TestChallenge2Unaffected 测试时，您已经完成了这个挑战。
修改您的解决方案，在配置转换进行中，副本组也可以立即开始提供分片服务。当您通过TestChallenge2Partial测试时，您已经完成了这个挑战。
**解决方案**
分片迁移以 group 为单位，这样即使一个 group挂了，也不会影响到另一个 group中的分片迁移。

上面的实验`ShardCtrler` 集群组实现了配置更新，分片均匀分配等任务，`ShardKVServer`则需要承载所有分片的读写任务，相比于MIT 6.824 Lab3 RaftKV的提供基础的读写服务，还需要功能为**配置更新，分片数据迁移，分片数据清理，空日志检测**。

## 客户端Clerk
主要请求逻辑:
- 使用key2shard()去找到一个 key 对应哪个分片Shard；
- 根据Shard从当前配置config中获取的 gid；
- 根据gid从当前配置config中获取 group 信息；
- 在group循环查找leaderId，直到返回请求成功、ErrWrongGroup或整个 group 都遍历请求过；
- Query 最新的配置，回到步骤1循环重复；

## 服务端Server
主要逻辑:
- 客户端首先和`ShardCtrler`交互，获取最新的配置，根据最新配置找到对应key的shard，请求该shard的group。
- 服务端`ShardKVServer`会创建多个 raft 组来承载所有分片的读写任务。
- 服务端`ShardKVServer`需要定期和`ShardCtrler`交互，保证更新到最新配置(monitor)。
- 服务端`ShardKVServer`需要根据最新配置完成配置更新，分片数据迁移，分片数据清理，空日志检测等功- 能。

