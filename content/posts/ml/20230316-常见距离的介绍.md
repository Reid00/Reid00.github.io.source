---
title: "常见距离的介绍"
date: 2023-03-16T19:35:21+08:00
lastmod: 2023-03-16T19:35:21+08:00
author: ["Reid"]
categories: 
- Machine Learning
- 机器学习
tags: 
- 欧式距离
- 距离
keyword:
- Machine Learning
- 机器学习
description: 常见距离的介绍
weight: # 输入1可以顶置文章，用来给文章展示排序，不填就默认按时间排序
slug: 常见距离的介绍
draft: false # 是否为草稿
comments: true
showToc: true # 显示目录
TocOpen: false # 自动展开目录
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: true # 底部不显示分享栏
showbreadcrumbs: true #顶部显示当前路径
cover:
    image: ""
    caption: ""
    alt: ""
    relative: false
---

## 机器学习常见距离介绍

### 1. 欧式距离

![欧氏距离](https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.1vblxn46yidc.webp)



### 2. 曼哈顿距离

我们可以定义曼哈顿距离的正式意义为L1-距离或城市区块距离，也就是在欧几里得空间的固定直角坐标系上两点所形成的线段对轴产生的投影的距离总和。例如在平面上，坐标（x1, y1）的点P1与坐标（x2, y2）的点P2的曼哈顿距离为：，要注意的是，曼哈顿距离依赖座标系统的转度，而非系统在座标轴上的平移或映射。
通俗来讲，想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。而实际驾驶距离就是这个“曼哈顿距离”，此即曼哈顿距离名称的来源， 同时，曼哈顿距离也称为城市街区距离(City Block distance)。

![曼哈顿距离](https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.7l95ss3go8s0.webp)

### 3. 切比雪夫距离

若二个向量或二个点p 、and q，其座标分别为p1,p2
![切比雪夫距离](https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.2uxwm77425u0.webp)

### 4. 闵可夫斯基距离(Minkowski Distance)

闵氏距离不是一种距离，而是一组距离的定义.

(1) 闵氏距离的定义       
两个n维变量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的闵可夫斯基距离定义为： 
![闵氏距离](https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.1oran7booq2o.webp)
其中p是一个变参数。
当p=1时，就是曼哈顿距离
当p=2时，就是欧氏距离
当p→∞时，就是切比雪夫距离       
根据变参数的不同，闵氏距离可以表示一类的距离。 

### 5. 标准化欧氏距离 (Standardized Euclidean distance )

标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，那先将各个分量都“标准化”到均值、方差相等。至于均值和方差标准化到多少，先复习点统计学知识。

假设样本集X的数学期望或均值(mean)为m，标准差(standard deviation，方差开根)为s，那么X的“标准化变量”X*表示为：(X-m）/s，而且标准化变量的数学期望为0，方差为1。

即，样本集的标准化过程(standardization)用公式描述就是：
![standard](https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.4g1qht1u37o0.webp)
标准化后的值 = ( 标准化前的值 － 分量的均值 ) /分量的标准差　　

经过简单的推导就可以得到两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的标准化欧氏距离的公式：　　
![d](https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.2fxxgizr03b4.webp)

如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧氏距离(Weighted Euclidean distance)。 

### 6. 马氏距离(Mahalanobis Distance)

有M个样本向量X1~Xm，[协方差矩阵](http://zh.wikipedia.org/wiki/协方差矩阵)记为S，均值记为向量μ，则其中样本向量X到u的马氏距离表示为： 
![马氏距离](https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.7jew7os09zs0.webp)

(协方差矩阵中每个元素是各个矢量元素之间的协方差Cov(X,Y)，Cov(X,Y) = E{ [X-E(X)] [Y-E(Y)]}，其中E为数学期望）

而其中向量Xi与Xj之间的马氏距离定义为：  
![向量化](https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.zs4emh5c39c.webp)

若协方差矩阵是单位矩阵（各个样本向量之间独立同分布）,则公式就成了：    
![matrix](https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.6mg0cl8qwrs0.webp)

也就是欧氏距离了。　　
若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。

- 马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰。 
- 「微博上的seafood高清版点评道：原来马氏距离是根据协方差矩阵演变，一直被老师误导了，怪不得看Killian在05年NIPS发表的LMNN论文时候老是看到协方差矩阵和半正定，原来是这回事」

### 7.巴氏距离（Bhattacharyya Distance）

在统计中，Bhattacharyya距离测量两个离散或连续概率分布的相似性。它与衡量两个统计样品或种群之间的重叠量的Bhattacharyya系数密切相关。Bhattacharyya距离和Bhattacharyya系数以20世纪30年代曾在印度统计研究所工作的一个统计学家A. Bhattacharya命名。同时，Bhattacharyya系数可以被用来确定两个样本被认为相对接近的，它是用来测量中的类分类的可分离性。

### 8.**汉明距离(Hamming distance)**

 两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。

### 9.**夹角余弦(Cosine)** 

几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。
![公式](https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.57am3ifvomw.webp)
 

### 0. 杰卡德相似系数(Jaccard similarity coefficient)

(1) 杰卡德相似系数    

两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。　
![jaccard](https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.4ownshmt9p40.webp)

**杰卡德相似系数是衡量两个集合的相似度一种指标。**

(2) 杰卡德距离 

与杰卡德相似系数相反的概念是杰卡德距离(Jaccard distance)。
杰卡德距离可用如下公式表示：
![公式](https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.175kxfhuwj1c.webp)

**杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度。**

(3) 杰卡德相似系数与杰卡德距离的应用   

可将杰卡德相似系数用在衡量样本的相似度上。
举例：样本A与样本B是两个n维向量，而且所有维度的取值都是0或1，例如：A(0111)和B(1011)。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。

M11 ：样本A与B都是1的维度的个数

​M01：样本A是0，样本B是1的维度的个数

​M10：样本A是1，样本B是0 的维度的个数

​M00：样本A与B都是0的维度的个数

![gs](https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.6s9n0w7zisw0.webp)

### **11.皮尔逊系数(Pearson Correlation Coefficient)**

在具体阐述皮尔逊相关系数之前，有必要解释下什么是相关系数 ( Correlation coefficient )与相关距离(Correlation distance)。

相关系数 ( Correlation coefficient )的定义是：

![cc](https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.dhfz7h5k3ts.webp)

(其中，E为数学期望或均值，D为方差，D开根号为标准差，E{ [X-E(X)] [Y-E(Y)]}称为随机变量X与Y的协方差，记为Cov(X,Y)，即Cov(X,Y) = E{ [X-E(X)] [Y-E(Y)]}，而两个变量之间的协方差和标准差的商则称为随机变量X与Y的相关系数，记为Pxy)
   相关系数衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。
    具体的，如果有两个变量：X、Y，最终计算出的相关系数的含义可以有如下理解：

1. 当相关系数为0时，X和Y两变量无关系。
2. 当X的值增大（减小），Y值增大（减小），两个变量为正相关，相关系数在0.00与1.00之间。
3. 当X的值增大（减小），Y值减小（增大），两个变量为负相关，相关系数在-1.00与0.00之间。

**(2)皮尔逊相关系数的适用范围**
当两个变量的标准差都不为零时，相关系数才有定义，皮尔逊相关系数适用于：

1. 两个变量之间是线性关系，都是连续数据。
2. 两个变量的总体是正态分布，或接近正态的单峰分布。
3. 两个变量的观测值是成对的，每对观测值之间相互独立。

**(3)皮尔逊相关的约束条件**

从以上解释, 也可以理解皮尔逊相关的约束条件:

1 两个变量间有线性关系
2 变量是连续变量
3 变量均符合正态分布,且二元分布也符合正态分布
4 两变量独立
在实践统计中,一般只输出两个系数,一个是相关系数,也就是计算出来的相关系数大小,在-1到1之间;另一个是独立样本检验系数,用来检验样本一致性

### Summary:

简单说来，各种“距离”的应用场景简单概括为，

空间：欧氏距离，

路径：曼哈顿距离，

国际象棋国王：切比雪夫距离，

以上三种的统一形式:闵可夫斯基距离，

加权：标准化欧氏距离，

排除量纲和依存：马氏距离，

向量差距：夹角余弦，

编码差别：汉明距离，

集合近似度：杰卡德类似系数与距离，

相关：相关系数与相关距离。