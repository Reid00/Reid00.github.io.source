---
title: "线性回归"
date: 2023-03-16T19:35:25+08:00
lastmod: 2023-03-16T19:35:25+08:00
author: ["Reid"]
categories: 
- Machine Learning
- 机器学习
tags: 
- 简单线性回归
keyword:
- Machine Learning
- 机器学习
description: 线性回归
weight: # 输入1可以顶置文章，用来给文章展示排序，不填就默认按时间排序
slug: 线性回归
draft: false # 是否为草稿
comments: true
showToc: true # 显示目录
TocOpen: false # 自动展开目录
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: true # 底部不显示分享栏
showbreadcrumbs: true #顶部显示当前路径
cover:
    image: ""
    caption: ""
    alt: ""
    relative: false
---

# 介绍

称函数为**效用函数** 线性回归模型看起来非常简单，简单到让人怀疑其是否有研究价值以及使用价值。但实际上，线性回归模型可以说是最重要的数学模型之一，很多模型都是建立在它的基础之上，可以被称为是“模型之母”。 

## 1.1 什么是简单线性回归

所谓简单，是指只有一个样本特征，即只有一个自变量；所谓线性，是指方程是线性的；所谓回归，是指用方程来模拟变量之间是如何关联的。

简单线性回归，其思想简单，实现容易（与其背后强大的数学性质相关。同时也是许多强大的非线性模型（多项式回归、逻辑回归、SVM）的基础。并且其结果具有很好的可解释性。

## 1.2  一种基本推导思

**我们所谓的建模过程，其实就是找到一个模型，最大程度的拟合我们的数据。** 在简单线回归问题中，模型就是我们的直线方程：y = ax + b 。

要想最大的拟合数据，本质上就是找到没有拟合的部分，也就是损失的部分尽量小，就是**损失函数**（loss function）（也有算法是衡量拟合的程度，称函数为**效用函数**（utility function））：

![image-20191201222429886](https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.hv31rx1fxaw.webp)

 

因此，推导思路为：

1. 通过分析问题，确定问题的损失函数或者效用函数；
2. 然后通过最优化损失函数或者效用函数，获得机器学习的模型

近乎所有参数学习算法都是这样的套路，区别是模型不同，建立的目标函数不同，优化的方式也不同。

回到简单线性回归问题，目标：

> 已知训练数据样本、 ，找到和的值，使 尽可能小

这是一个典型的最小二乘法问题（最小化误差的平方）

通过最小二乘法可以求出a、b的表达式：

![image-20191201222456542](https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.1j8ecbere97k.webp)

 

##  最小二乘法

### 2.1 由损失函数引出一堆“风险”

#### 2.1.1 损失函数

在机器学习中，所有的算法模型其实都依赖于**最小化或最大化某一个函数**，我们称之为“**目标函数**”。

最小化的这组函数被称为“损失函数”。什么是损失函数呢？

> 损失函数描述了单个样本预测值和真实值之间误差的程度。用来度量模型一次预测的好坏。

损失函数是衡量预测模型预测期望结果表现的指标。损失函数越小，模型的鲁棒性越好。。

常用损失函数有：

- 0-1损失函数：用来表述分类问题，当预测分类错误时，损失函数值为1，正确为![image-20191201222555831](https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.1rfowr7oke68.webp)

 

- 平方损失函数：用来描述回归问题，用来表示连续性变量，为预测值与真实值差值的平方。（误差值越大、惩罚力度越强，也就是对差值敏感）

![image-20191201222620710](https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.5jdjsn40gn40.webp)

 

- 绝对损失函数：用在回归模型，用距离的绝对值来衡量

![image-20191201222638485](https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.22fwss0ekwu8.webp)

 

- 对数损失函数：是预测值Y和条件概率之间的衡量。事实上，该损失函数用到了极大似然估计的思想。P(Y|X)通俗的解释就是：在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。由于概率之间的同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。最后由于是损失函数，所以预测正确的概率越高，其损失值应该是越小，因此再加个负号取个反。



 ![image-20191201222700230](https://raw.githubusercontent.com/Reid00/image-host/main/20220607/image.391e0sb2yfs0.webp)

以上损失函数是针对于单个样本的，但是一个训练数据集中存在N个样本，N个样本给出N个损失，如何进行选择呢？

这就引出了风险函数。

#### 2.1.2 期望风险

**期望风险**是**损失函数的期望**，用来表达**理论上模型f(X)关于联合分布P(X,Y)的平均意义下的损失**。又叫**期望损失/风险函数**。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXOJgib78PN0uZ9iaStQtXNorYAVAkaa7o0R6kgHEqctiaJD1lhRziay1Igw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



#### 2.1.3 经验风险

**模型f(X)关于训练数据集的平均损失，称为经验风险或经验损失**。

其公式含义为：模型关于训练集的平均损失（每个样本的损失加起来，然后平均一下）

![img](https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXf4a5qURt6g4oibVq5jObibUfbw6eiajyjiaePqwIUiaSRp3Q5fqoicjPaTAw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**经验风险最小的模型为最优模型**。在训练集上最小经验风险最小，也就意味着预测值和真实值尽可能接近，模型的效果越好。公式含义为取训练样本集中对数损失函数平均值的最小。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXSKiaGFyJkS3alZrnQMicpwy6UbiajJa2Ff7MLdxmaUov0hwk8KMrXyzLQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



#### 2.1.4 经验风险最小化和结构风险最小化

期望风险是模型关于联合分布的期望损失，经验风险是模型关于训练样本数据集的平均损失。根据大数定律，**当样本容量N趋于无穷时，经验风险趋于期望风险。**

因此很自然地想到**用经验风险去估计期望风险**。但是由于训练样本个数有限，可能会出现过度拟合的问题，即决策函数对于训练集几乎全部拟合，但是对于测试集拟合效果过差。因此需要对其进行矫正：

- **结构风险最小化**：当样本容量不大的时候，经验风险最小化容易产生“过拟合”的问题，为了“减缓”过拟合问题，提出了**结构风险最小**理论。结构风险最小化为经验风险与复杂度同时较小。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXcZTgYiaFzLRBl6ibchic17icZVFfXvXsKT1Ooq4LHuVrAbXvBUPSrYUnJw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

通过公式可以看出，结构风险：**在经验风险上加上一个正则化项**(regularizer)，或者叫做罚项(penalty) 。正则化项是J(f)是函数的复杂度再乘一个权重系数（用以权衡经验风险和复杂度）

#### 2.1.5 小结

1、损失函数：单个样本预测值和真实值之间误差的程度。

2、期望风险：是损失函数的期望，理论上模型f(X)关于联合分布P(X,Y)的平均意义下的损失。

3、经验风险：模型关于训练集的平均损失（每个样本的损失加起来，然后平均一下）。

4、结构风险：在经验风险上加上一个正则化项，防止过拟合的策略。

### 2.2 最小二乘法

#### 2.2.1 什么是最小二乘法

言归正传，进入最小二乘法的部分。

大名鼎鼎的最小二乘法，虽然听上去挺高大上，但是思想还是挺朴素的，符合大家的直觉。

最小二乘法源于法国数学家阿德里安的猜想：

> 对于测量值来说，让总的误差的平方最小的就是真实值。这是基于，如果误差是随机的，应该围绕真值上下波动。

即：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrX1bXZfKEgic6ibyNKnw7oMxg7icUicYwXtBEQUYa3qYcuRia5PLib91QHibVdg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

那么为了求出这个二次函数的最小值，对其进行求导，导数为0的时候取得最小值：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXH9P6xNN2ZY3et4OiaZeYJ1Am04bM9rP1t7xzlNeCuSod0E300S4tBlQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

进而：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXBXn2IN5Z3gAQMFegK2coPyVGu9ktjRyeViaukOCg7Bj9xcmLBA9lSHQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

正好是算数平均数（算数平均数是最小二乘法的特例）。

这就是最小二乘法，所谓“二乘”就是平方的意思。

（高斯证明过：如果误差的分布是正态分布，那么最小二乘法得到的就是最有可能的值。）

#### 2.2.2 线性回归中的应用

我们在第一章中提到：

> 目标是，找到a和b，使得损失函数：尽可能的小。

这里，将简单线性问题转为最优化问题。下面对函数的各个位置分量求导，导数为0的地方就是极值：

对 进行求导：



 

然后mb提到等号前面，两边同时除以m，等号右面的每一项相当于均值。



 

现在 对 进行求导：



 

此时将对 进行求导得到的结果 代入上式中，得到： 

将上式进行整理，得到

![img](https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXW2rQPAy5ibrfGmBz7rpd36WY8IZSl27LZsMEjPwew7gTiacEzTC7AiaPw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



将上式继续进行整理：

![img](https://mmbiz.qpic.cn/mmbiz_jpg/1fsH49VZrGEyleqiaSavoviamvhHGQoDrXHiav4h8wZdciagf6f584U20MqHdN2JibwEqXnXOtTxF9Yoibiaricibs3k5Bg/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这样在实现的时候简单很多。

最终我们通过最小二乘法得到a、b的表达式：



 

## 总结

本章中，我们从数学的角度了解了简单线性回归，从中总结出一类机器学习算法的基本思路：

1. 通过分析问题，确定问题的损失函数或者效用函数；
2. 然后通过最优化损失函数或者效用函数，获得机器学习的模型。

理解了损失函数的概念，并列举出了常见损失函数，并引出了一堆“风险”。最后为了求出最小的损失函数，学习了最小二乘法，并进行了完整的数学推导。

下一篇，我们将会实现简单线性回归，并添加到我们自己的工程文件里。