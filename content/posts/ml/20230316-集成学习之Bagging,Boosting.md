---
title: "集成学习之Bagging,Boosting"
date: 2023-03-16T19:35:27+08:00
lastmod: 2023-03-16T19:35:27+08:00
author: ["Reid"]
categories: 
- Machine Learning
- 机器学习
tags: 
- 集成学习
- Bagging
- Boosting
keyword:
- Machine Learning
- 机器学习
description: 集成学习之Bagging,Boosting
weight: # 输入1可以顶置文章，用来给文章展示排序，不填就默认按时间排序
slug: 集成学习之Bagging,Boosting
draft: false # 是否为草稿
comments: true
showToc: true # 显示目录
TocOpen: false # 自动展开目录
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: true # 底部不显示分享栏
showbreadcrumbs: true #顶部显示当前路径
cover:
    image: ""
    caption: ""
    alt: ""
    relative: false
---


#### 生成子模型的两种取样方式

那么为了造成子模型之间的差距，每个子模型只看样本中的一部分，这就涉及到两种取样方式：

- 放回取样：Bagging，在统计学中也被称为bootstrap。
- 不放回取样：Boosting

在集成学习中我们通常采用 Bagging 的方式，具体原因如下：

- 因为取样后放回，所以不受样本数据量的限制，允许对同一种分类器上对训练集进行进行多次采样，可以训练更多的子模型。
- 在 train_test_split 时，不那么强烈的依赖随机；而 Boosting的方式，会受到随机的影响；
- Boosting的随机问题：Pasting 的方式等同于将 500 个样本分成 5 份，每份 100 个样本，怎么分，将对子模型有较大影响，进而对集成系统的准确率有较大影响。

#### 什么是Bagging

Bagging，即`bootstrap aggregating`的缩写，每个训练集称为`bootstrap`。

**Bagging是一种根据均匀概率分布从数据中重复抽样（有放回）的技术** 。

Bagging能提升机器学习算法的稳定性和准确性，它可以减少模型的方差从而避免overfitting。它通常应用在决策树方法中，其实它可以应用到任何其它机器学习算法中。

Bagging方法在不稳定模型（unstable models）集合中表现比较好。这里说的不稳定的模型，即在训练数据发生微小变化时产生不同泛化行为的模型（高方差模型），如决策树和神经网络。

但是Bagging在过于简单模型集合中表现并不好，因为Bagging是从总体数据集随机选取样本来训练模型，过于简单的模型可能会产生相同的预测结果，失去了多样性。

总结一下Bagging方法：

1. Bagging通过降低基分类器的方差，改善了泛化误差
2. 其性能依赖于基分类器的稳定性；如果基分类器不稳定，bagging有助于降低训练数据的随机波动导致的误差；如果稳定，则集成分类器的误差主要由基分类器的偏差引起
3. 由于每个样本被选中的概率相同，因此bagging并不侧重于训练数据集中的任何特定实例



#### Bagging的使用

sklearn为Bagging提供了一个简单的API：BaggingClassifier类（回归是BaggingRegressor）。首先需要传入一个模型作为参数，可以使用决策树；然后需要传入参数`n_estimator`即集成多少个子模型；参数`max_samples`表示每次从数据集中取多少样本；参数`bootstrap`设置为True表示使用有放回取样Bagging，设置为False表示使用无放回取样Pasting。可以通过`n_jobs`参数来分配训练所需CPU核的数量，-1表示会使用所有空闲核（集成学习思路，极易并行化处理）。

bagging是不能减小模型的偏差的，因此我们要选择具有低偏差的分类器来集成，例如：没有修剪的决策树。

Bootstrap 在每个预测器被训练的子集中引入了更多的分集，所以 Bagging 结束时的偏差比 Pasting 更高，但这也意味着预测因子最终变得不相关，从而减少了集合的方差。总体而言，Bagging 通常会导致更好的模型，这就解释了为什么它通常是首选的。然而，如果你有空闲时间和 CPU 功率，可以使用交叉验证来评估 Bagging 和 Pasting 哪一个更好。

#### Out-of-Bag

对于Bagging来说，一些实例可能被一些分类器重复采样，但其他的有可能不会被采样。由于每个bootstrap的M个样本是有放回随机选取的，因此每个样本不被选中的概率为。当N和M都非常大时，比如N=M=10000，一个样本不被选中的概率p = 36.8%。因此一个bootstrap约包含原样本63.2%，约36.8%的样本未被选中。这些没有被采样的训练实例就叫做Out-of-Bag实例。但注意对于每一个的分类器来说，它们各自的未选中部分不是相同的。

那么这些未选中的样本有什么用呢？

因为在训练中分类器从来没有看到过Out-of-Bag实例，所以它可以在这些样本上进行预测，就不用分样本测试集和测试数据集了。

在sklearn中，可以在训练后需要创建一个`BaggingClassifier`时设置`oob_score=True`来进行自动评估。

```python
bagging_clf = BaggingClassifier(DecisionTreeClassifier(),
                               n_estimators=5000, max_samples=100,
                               bootstrap=True, oob_score=True)
bagging_clf.fit(X, y)
bagging_clf.oob_score_
```

#### 另一种差异化方式：针对特征取样

我们上面提到的，都是通过对样本进行取样，来得到差异化的子模型。除了取部分样本以外，还可以针对特征进行随机取样。尤其是在样本特征非常多的情况下时，如图像领域中，每个像素点都是一个特征，则Bagging时可以对特征进行取样。

在BaggingClassifier中有两个超参数，`max_features`表示每次取多少特征；`bootstrap_features`设置为True则开启。

在Bagging中，所有的分类器都可以是并行训练的。与之相对应的，串行训练的`Boosting`也是集成训练中的一大类别。接下来我们会介绍`Boosting`算法。

#### Boosting思想

Boosting是增强、推动的意思。它是一种迭代的方法，各个子模型彼此之间不是独立的关系，而是相互增强（Boosting）的关系。每一次训练的时候都更加关心分类错误的样例，给这些分类错误的样例增加更大的权重，下一次迭代的目标就是能够更容易辨别出上一轮分类错误的样例。每个模型都在尝试增强整体的效果，最终将这些弱分类器进行加权相加。

**因此与Bagging中各学习器并行处理不同的是：Boosting是串行的，环环相扣且有先后顺序的**

#### Boosting工作机制

Boosting的工作流程:

现有原始数据集，首先挑出一些数据，然后在上训练分类器得到。然后用在原始数据集上测试一下，看哪些样本分类对了，哪些样本分类错了。然后**把分错的和分对的分别挑出一部分**，组成新的数据集（也就是说，**刻意筛出有对有错的数据集**）。再使用某分类器训练数据集，**即专门地、有目的性地学习D1数据集哪些学对了、哪些学错了**，得到C2.

有了C1, C2 两个分类器都在原始数据集D上进行测试，目的是找到C1、C2**结果不一致的样本**，组成一个新的数据集D3，再用一个分类器训练D3，得到（**专门用来解决C1、C2的争端**）。

其算法流程为：

1. 先从初始训练集训练出一个弱学习器；
2. 再**根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注**；
3. 然后基于调整后的**样本分布**来训练下一个基学习器；
4. 如此重复进行，直至基学习器数目达到事先指定的值，最终将这个基学习器进行**加权结合**。使后续模型应该能够补偿早期模型所造成的错误。

先训练一个分类器，根据这个分类器的误差将训练样本重新调整，也就是加权重。原来的每个样本有同样的机会去作为训练样本，在某个样本上犯的错误越多，其权重也就越大。这很好理解，就是让后面的分类器去重点学习前面分错的样本。

### Boosting和Bagging的区别（重要总结）

1、样本选择上：

- Bagging：训练集是在原始集中**有放回**选取的，从原始集中选出的**各轮训练集之间是独立**的。
- Boosting：每一轮的**训练集不变**，只是训练集中**每个样例在分类器中的权重**发生变化。而权值是根据上一轮的分类结果进行调整。

2、样例权重：

- Bagging：使用均匀取样，每个样例的**权重相等**。
- Boosting：根据错误率不断调整样例的权值，**错误率越大则权重越大**。

3、分类器权重：

- Bagging：所有**分类器的权重相等**。
- Boosting：每个弱分类器都有相应的权重，对于**分类误差小的分类器会有更大的权重**。

4、并行&串行：

- Bagging：各个预测函数可以**并行**生成。
- Boosting：各个预测函数只能**顺序生成**，因为后一个模型参数需要前一轮模型的结果。

#### **偏差和方差（重要！）**

偏差（bias）衡量了模型的**预测值与实际值之间的偏离关系**，反映了模型本身的拟合能力；方差（variance）描述的是训练数据在不同迭代阶段的训练模型中，**预测值的变化波动情况（或称之为离散情况）**。

Bagging用多个分类器进行并行训练的目的就是**降低方差**。因为相互独立的分类器多了，就会让目标值更聚合。

而对于Boosting来说，每一轮都针对于上一轮进行学习，力求准确，即可以**降低偏差（残差）**。

因此，**在实际使用时，我们就要考虑模型的特性。对于方差低的Bagging（随机森林）来说，采用深度较深且不剪枝的决策树，以此降低其偏差。对于偏差低的Boosting（GBDT）要选简单的、深度浅的决策树。**

在Bagging方法中各个基体学习器之间不存在依赖关系，集成多个模型，综合有差异的子模型，融合出比较好的模型，且可以并行处理。如随机森林算法（RF）等。

而Boosting方法必须串行生成，各个基学习器**存在依赖关系,基于前面模型的训练结果误差生成新的模型**，代表的算法有：Adaboost、GBDT、XGBoost等。