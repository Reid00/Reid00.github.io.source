---
title: "逻辑回归的常见面试题总结"
date: 2022-06-08T09:27:08+08:00
lastmod: 2022-06-08T09:27:08+08:00
author: ["Reid"]
categories: 
- Machine Learning
tags: 
- 面试
- 逻辑回归
- LR
keyword:
- Machine Learning
- 机器学习
- LR
description: ""
weight: # 输入1可以顶置文章，用来给文章展示排序，不填就默认按时间排序
slug: ""
draft: false # 是否为草稿
comments: true
showToc: true # 显示目录
TocOpen: false # 自动展开目录
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: true # 底部不显示分享栏
showbreadcrumbs: true #顶部显示当前路径
cover:
    image: ""
    caption: ""
    alt: ""
    relative: false
---

### 1.简介

逻辑回归是面试当中非常喜欢问到的一个机器学习算法，因为表面上看逻辑回归形式上很简单，很好掌握，但是一问起来就容易懵逼。所以在面试的时候给大家的第一个建议不要说自己精通逻辑回归，非常容易被问倒，从而减分。下面总结了一些平常我在作为面试官面试别人和被别人面试的时候，经常遇到的一些问题。

Regression问题的常规步骤为：
1. 寻找h函数（即假设估计的函数）；
2. 构造J函数（损失函数）；
3. 想办法使得J函数最小并求得回归参数（θ）；
4. 数据拟合问题


### 2.正式介绍

   如何凸显你是一个对逻辑回归已经非常了解的人呢。那就是用一句话概括它！**逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。**

   这里面其实包含了5个点 1：逻辑回归的假设，2：逻辑回归的损失函数，3：逻辑回归的求解方法，4：逻辑回归的目的，5:逻辑回归如何分类。这些问题是考核你对逻辑回归的基本了解。

#### 逻辑回归的基本假设

- - 任何的模型都是有自己的假设，在这个假设下模型才是适用的。逻辑回归的**第一个**基本假设是**假设数据服从伯努利分布。**伯努利分布有一个简单的例子是抛硬币，抛中为正面的概率是pp,抛中为负面的概率是1−p1−p.在逻辑回归这个模型里面是假设 hθ(x)hθ(x) 为样本为正的概率，1−hθ(x)1−hθ(x)为样本为负的概率。那么整个模型可以描述为

    hθ(x;θ)=phθ(x;θ)=p

  - 逻辑回归的第二个假设是假设样本为正的概率是 

    

    p=11+e−θTxp=11+e−θTx

- - 所以逻辑回归的最终形式 

    hθ(x;θ)=11+e−θTx

#### 逻辑回归的求解方法

- 由于该极大似然函数无法直接求解，我们一般通过对该函数进行梯度下降来不断逼急最优解。在这个地方其实会有个加分的项，考察你对其他优化方法的了解。因为就梯度下降本身来看的话就有随机梯度下降，批梯度下降，small batch 梯度下降三种方式，面试官可能会问这三种方式的优劣以及如何选择最合适的**梯度下降**方式。

  - 简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。

  - 随机梯度下降是以高方差频繁更新，优点是使得sgd（随机梯度下降）会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。

    如果使用梯度下降法(批量梯度下降法)，那么**每次迭代过程中都要对** ![[公式]](https://www.zhihu.com/equation?tex=n)**个样本进行求梯度**，所以开销非常大，**随机梯度下降的思想就是随机采样一个样本** ![[公式]](https://www.zhihu.com/equation?tex=J%28x_i%29)**来更新参数**，那么计算开销就从 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%7B%28n%29%7D) 下降到 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%7B%281%29%7D) 。

    随机梯度下降虽然提高了计算效率，降低了计算开销，但是由于每次迭代只随机选择一个样本，**因此随机性比较大，所以下降过程中非常曲折**

    可以看到多了随机两个字，随机也就是说我们用样本中的一个例子来近似我所有的样本，来调整*θ*，因而随机梯度下降是会带来一定的问题，因为计算得到的并不是准确的一个梯度，**对于最优化问题，凸问题，**虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。

  - 小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。小批量梯度下降的开销为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%7B%28%5Cleft%7C+%5Cmathscr%7BB%7D+%5Cright%7C%29%7D) 其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%7C+%5Cmathscr%7BB%7D+%5Cright%7C)是批量大小。

- - 其实这里还有一个隐藏的更加深的加分项，看你了不了解诸如Adam，动量法等优化方法。因为上述方法其实还有两个致命的问题。
    - 第一个是如何对模型选择合适的学习率。自始至终保持同样的学习率其实不太合适。因为一开始参数刚刚开始学习的时候，此时的参数和最优解隔的比较远，需要保持一个较大的学习率尽快逼近最优解。但是学习到后面的时候，参数和最优解已经隔的比较近了，你还保持最初的学习率，容易越过最优点，在最优点附近来回振荡，通俗一点说，就很容易学过头了，跑偏了。
    - 第二个是如何对参数选择合适的学习率。在实践中，对每个参数都保持的同样的学习率也是很不合理的。有些参数更新频繁，那么学习率可以适当小一点。有些参数更新缓慢，那么学习率就应该大一点。这里我们不展开，有空我会专门出一个专题介绍。

#### 逻辑回归的目的

- 该函数的目的便是将数据二分类，提高准确率。

#### 逻辑回归如何分类

- 逻辑回归作为一个回归(也就是y值是连续的)，如何应用到分类上去呢。y值确实是一个连续的变量。逻辑回归的做法是划定一个阈值，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。

#### 逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？

- 损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。至于原因大家可以求出这个式子的梯度更新

  这个式子的更新速度只和相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。

- 为什么不选平方损失函数的呢？其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。

#### **逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？**

先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。

但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一

如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。

#### **为什么我们还是会在训练的过程当中将高度相关的特征去掉？**

- 去掉高度相关的特征会让模型的可解释性更好
- 可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。

#### **4.逻辑回归的优缺点总结**

**优点**

- 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。

- 模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。

- 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。

- 资源占用小,尤其是内存。因为只需要存储各个维度的特征值，。

- 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。

**但是逻辑回归本身也有许多的缺点:**

- 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。

- 很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。

- 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。

- 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。

#### **怎么防止过拟合？**

通过正则化方法。正则化方法是指在进行目标函数或代价函数优化时，在目标函数或代价函数后面加上一个正则项，一般有L1正则与L2正则等

#### **为什么正则化可以防止过拟合？**

过拟合表现在训练数据上的误差非常小，而在测试数据上误差反而增大。其原因一般是模型过于复杂，过分得去拟合数据的噪声**。正则化则是对模型参数添加先验，使得模型复杂度较小，对于噪声扰动相对较小**。

最简单的解释就是加了先验。在数据少的时候，先验知识可以防止过拟合。

举个例子：

硬币，推断正面朝上的概率。如果只能抛5次，很可能5次全正面朝上，这样你就得出错误的结论：正面朝上的概率是1--------过拟合！如果你在模型里加正面朝上概率是0.5的先验，结果就不会那么离谱。这其实就是正则

#### **L1正则和L2正则有什么区别？**

相同点：都用于避免过拟合

不同点：**L2与L1的区别在于，L1正则是拉普拉斯先验，而L2正则则是高斯先验**。L1可以产生稀疏解,可以让一部分特征的系数缩小到0，从而间接实现特征选择。所以L1适用于特征之间有关联的情况。L2让所有特征的系数都缩小，但是不会减为0，它会使优化求解稳定快速。所以L2适用于特征之间没有关联的情况

因为L1和服从拉普拉斯分布，所以L1在0点处不可导，难以计算，这个方法可以使用Proximal Algorithms或者ADMM来解决。

#### **如何用LR解决非线性问题？**

将特征离散成高维的01特征可以解决分类模型的非线性问题

### 3. 逻辑回归是线性模型么，说下原因？

![img](https://raw.githubusercontent.com/Reid00/image-host/main/20220608/image.5phf4bim80g0.webp)

狭义线性模型的前提是因变量误差是正态分布，但很多情况下这并不满足，比如对足球比分的预测显然用泊松分布是更好的选择。而广义的”广”在于引入了联系函数，于是误差变成了只要满足指数分布族就行了，因此适用性更强。 

​    简单来说广义线性模型分为两个部分，第一个部分是描述了自变量和因变量的系统关系，也就是”线性”所在；第二个部分是描述了因变量的误差，这可以建模成各种满足指数分布族的分布。而联系函数就是把这两个部分连接起来的桥梁，也就是把因变量的期望表示为了自变量线性组合的函数。而像逻辑回归这样的简单广义线性模型，实际是将自变量的线性组合变成了联系函数的自然参数，这类联系函数也可以叫做正则联系函数。 

## 4. 逻辑回归算法为什么用的是sigmoid函数而不用阶跃函数？

 阶跃函数虽然能够直观刻画分类的错误率，但是由于其非凸、非光滑的特点，使得算法很难直接对该函数进行优化。而sigmoid函数本身的特征（光滑无限阶可导），以及完美的映射到概率空间，就用于逻辑回归了。解释上可从三个方面：- 最大熵定理- 伯努利分布假设- 贝叶斯理论 

 参考:  https://fengxc.me/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E7%82%B9-%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E4%B8%AD.html